Using S3 Storage
================


Triv.io let's you  structure unstructured data in s3, we provide a datasource that makes this very simple if you follow  a couple of conventions  which we  are described in this document. If these conventions don't work for you,  you're free to create your own datasource that adapts your s3 layout to work with triv.io.


Convention 1: The s3 key prefix is used to pass additional prameters to a job. 
---

The form  of an s3 key looks like this `s3://<bucket name>/<table name>/<key1>=<value1>/<key2>=<value2>/<unique blob name>`



Imagine  you  have 4 csv files each with 1,000 rows in it. You would save these to s3 like this.

```
s3://mybucket/myfoo/dt=2012-05-31/blob1.csv
s3://mybucket/myfoo/dt=2012-05-31/blob2.csv
s3://mybucket/myfoo/dt=2012-05-31/blob3.csv
```

Your map job would receive each row from the csv files as an input, the key=value, `dt=2012-05-31`, in the prefix will be available as the 'dt' attribute of the parameter object passed to the map function. You can use this as you see fit in your script. Here's an example outputing `params.dt` as the key and using column 0 of the record as your value.

```
  def map(csvrow, params):
    # yield the column 2 as the value and 
    yield  param.dt, csvrow[0]
    
  ... insert your clever reduce method here ...
```

The `<key>=<value>` pairs allow you to pass additional data to your jobs that may not be available as the content of your files. For example say these csv are for  a specific customer and there is no customer attribute in your file. If you  save the files to s3 like this:


```
s3://mybucket/myfoo/dt=2012-05-31/customer_id=123/blob1.csv
s3://mybucket/myfoo/dt=2012-05-31/customer_id=123/blob2.csv
s3://mybucket/myfoo/dt=2012-05-31/customer_id=546/blob3.csv
```


Then `params.customer_id` will be either **123** or **546** for the rows that come out of the corresponding blobs.

Convention 2: dt=YYYY-MM-DD[HH:mm:SS] is required and must be the first `<key>=<value>` in the s3 prefix.
---

triv.io automatically batches all blobs with the same `dt=<datetime>` into the same job. Without this key trivio does not know how to sgement your data


Convention 3: Your blob name must be unique
---

It bears mentioning that you are responsible for uniquely naming each blob. If you give two blobs the same name you'll clobber contents of at least one of them when you post to s3. Common strategies for keeping blobs uniquely named are to either post a file using a UUID library or naming the file with the host name and time that the file was generated. 

Examples:

```
s3://mybucket/myfoo/dt=2012-05-31/customer_id=123/2D5D76A4-5D28-4D22-9D03-6898E3E0FCD6.csv
```

or 

```
s3://mybucket/myfoo/dt=2012-05-31/customer_id=123/host1-1349886440.665647.csv
```


Convention 4: triv.io will parse the content according to the blobs extension.
---


By default triv.io will use the extension of the blob key to determine the file's content and parse it accordingly. In the above example triv.io will determine that the files contain comma separated values because they end with ".csv" parse each row and call you're map script with it. If you post the file with a ".json" extension it would be interpreted each line of the file json as a json record. You can override this behavior in the triv.io script by specifying a specific mimetype to use and even create your own mimeypes reader as part of your script if the default behaviors don't work for you.



